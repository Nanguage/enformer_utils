{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/cuda/lib/python3.10/site-packages/tqdm-4.65.0-py3.10.egg/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from enformer_pytorch import Enformer\n",
    "from enformer_pytorch.finetune import HeadAdapterWrapper\n",
    "import sys; sys.path.append('../')\n",
    "from enformer_utils.data.bigwig import BigwigFetcher\n",
    "from enformer_utils.data.fasta import FastaFetcher\n",
    "from enformer_utils.data.bed import BedIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr  6 03:14:35 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1080         Off| 00000000:02:00.0 Off |                  N/A |\n",
      "| 35%   45C    P0               41W / 180W|      0MiB /  8192MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce GTX 1080 Ti      Off| 00000000:83:00.0 Off |                  N/A |\n",
      "|  0%   38C    P5               16W / 280W|      0MiB / 11264MiB |      1%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current GPU:  NVIDIA GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0')\n",
    "torch.cuda.set_device(device=device)\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the device name\n",
    "    device_name = torch.cuda.get_device_name(torch.cuda.current_device())\n",
    "    print('The current GPU: ', device_name)\n",
    "else:\n",
    "    print('GPU is not aviailable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the data files for training\n",
    "bigwig_files = [\n",
    "    \"../data/LW-2W-1-Muscle-H3K4me3-AD1_merge_LW-2W-2-Muscle-H3K4me3-AD7.nodup.tagAlign_x_LW-2W-2-Muscle-Input-AD9_merge_LW-2W-1-Muscle-Input-AD13.nodup.tagAlign.pval.signal.bw\",\n",
    "    \"../data/MS-2W-2-Muscle-H3K27ac-AD1_merge_MS-2W-4-Muscle-H3K27ac-AD22.nodup.tagAlign_x_MS-2W-2-Muscle-Input-AD12-1_merge_MS-2W-4-Muscle-Input-AD25.nodup.tagAlign.pval.signal.bw\",\n",
    "]\n",
    "\n",
    "fa_file = \"../data/Sus_scrofa.Sscrofa11.1.dna.toplevel.chr.fa\"\n",
    "target_region_bed = \"../data/target_regions.bed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "enformer = Enformer.from_pretrained('EleutherAI/enformer-official-rough')\n",
    "\n",
    "model = HeadAdapterWrapper(\n",
    "    enformer = enformer,\n",
    "    num_tracks = len(bigwig_files),\n",
    "    post_transformer_embed = False,\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, chr1:0-98304, loss: 2.1053252595515883\n",
      "Epoch: 0, chr1:98304-196608, loss: 1.7726745579496668\n",
      "Epoch: 0, chr1:196608-294912, loss: 3.0932596674492374\n",
      "Epoch: 0, chr1:294912-393216, loss: 0.9147566630917908\n",
      "Epoch: 0, chr1:393216-491520, loss: 1.121840956972941\n",
      "Epoch: 0, chr1:491520-589824, loss: 3.039342317576095\n",
      "Epoch: 0, chr1:589824-688128, loss: 0.8242230878449354\n",
      "Epoch: 0, chr1:688128-786432, loss: 0.861079661131393\n",
      "Epoch: 0, chr1:786432-884736, loss: 1.4838105301601985\n",
      "Epoch: 0, chr1:884736-983040, loss: 0.9021866240476862\n",
      "Epoch: 0, chr1:983040-1081344, loss: 0.8179367523388418\n",
      "Epoch: 0, chr1:1081344-1179648, loss: 0.8174810998310738\n",
      "Epoch: 0, chr1:1179648-1277952, loss: 0.8499894725652004\n",
      "Epoch: 0, chr1:1277952-1376256, loss: 1.1409548687903963\n",
      "Epoch: 0, chr1:1376256-1474560, loss: 0.8734160058737024\n",
      "Epoch: 0, chr1:1474560-1572864, loss: 1.1333819258690778\n",
      "Epoch: 0, chr1:1572864-1671168, loss: 1.1755563072518558\n",
      "Epoch: 0, chr1:1671168-1769472, loss: 1.6620199251723415\n",
      "Epoch: 0, chr1:1769472-1867776, loss: 1.0238393437137352\n",
      "Epoch: 0, chr1:1867776-1966080, loss: 0.8232011589080547\n",
      "Epoch: 0, chr1:1966080-2064384, loss: 0.8082980025237697\n",
      "Epoch: 0, chr1:2064384-2162688, loss: 1.5152601862081716\n",
      "Epoch: 0, chr1:2162688-2260992, loss: 0.8612329490635403\n",
      "Epoch: 0, chr1:2260992-2359296, loss: 1.3885235084623821\n",
      "Epoch: 0, chr1:2359296-2457600, loss: 1.3543267458209964\n",
      "Epoch: 0, chr1:2457600-2555904, loss: 1.3473569120584088\n",
      "Epoch: 0, chr1:2555904-2654208, loss: 2.180243862343094\n",
      "Epoch: 0, chr1:2654208-2752512, loss: 1.0877811290778112\n",
      "Epoch: 0, chr1:2752512-2850816, loss: 1.3981192859472062\n",
      "Epoch: 0, chr1:2850816-2949120, loss: 0.9143936870650486\n",
      "Epoch: 0, chr1:2949120-3047424, loss: 0.7340063418360285\n",
      "Epoch: 0, chr1:3047424-3145728, loss: 2.238149786338909\n",
      "Epoch: 0, chr1:3145728-3244032, loss: 0.823384078215432\n",
      "Epoch: 0, chr1:3244032-3342336, loss: 0.8063232557620702\n",
      "Epoch: 0, chr1:3342336-3440640, loss: 0.8810688230891214\n",
      "Epoch: 0, chr1:3440640-3538944, loss: 0.8218289233572645\n",
      "Epoch: 0, chr1:3538944-3637248, loss: 0.7352492482351214\n",
      "Epoch: 0, chr1:3637248-3735552, loss: 0.7440879652826541\n",
      "Epoch: 0, chr1:3735552-3833856, loss: 0.7470253971699898\n",
      "Epoch: 0, chr1:3833856-3932160, loss: 0.7606323422915982\n",
      "Epoch: 0, chr1:3932160-4030464, loss: 0.7416599382427671\n",
      "Epoch: 0, chr1:4030464-4128768, loss: 0.7321354508538994\n",
      "Epoch: 0, chr1:4128768-4227072, loss: 0.7840504085502622\n",
      "Epoch: 0, chr1:4227072-4325376, loss: 1.6486058439147593\n",
      "Epoch: 0, chr1:4325376-4423680, loss: 1.3497167679214785\n",
      "Epoch: 0, chr1:4423680-4521984, loss: 1.4127171142763433\n",
      "Epoch: 0, chr1:4521984-4620288, loss: 1.5399572840609193\n",
      "Epoch: 0, chr1:4620288-4718592, loss: 0.81361483947419\n",
      "Epoch: 0, chr1:4718592-4816896, loss: 0.8352047297124368\n",
      "Epoch: 0, chr1:4816896-4915200, loss: 3.4310679406291156\n",
      "Epoch: 0, chr1:4915200-5013504, loss: 1.643879989229037\n",
      "Epoch: 0, chr1:5013504-5111808, loss: 0.8501872148078674\n",
      "Epoch: 0, chr1:5111808-5210112, loss: 0.7106375383409315\n",
      "Epoch: 0, chr1:5210112-5308416, loss: 0.7128432772574821\n",
      "Epoch: 0, chr1:5308416-5406720, loss: 0.7229091966182457\n",
      "Epoch: 0, chr1:5406720-5505024, loss: 1.5063439737467001\n",
      "Epoch: 0, chr1:5505024-5603328, loss: 0.8029344874833492\n",
      "Epoch: 0, chr1:5603328-5701632, loss: 0.786403863545734\n",
      "Epoch: 0, chr1:5701632-5799936, loss: 0.7376883435650128\n",
      "Epoch: 0, chr1:5799936-5898240, loss: 0.7971612126775626\n",
      "Epoch: 0, chr1:5898240-5996544, loss: 0.7978803355982853\n",
      "Epoch: 0, chr1:5996544-6094848, loss: 0.8291698490084737\n",
      "Epoch: 0, chr1:6094848-6193152, loss: 0.792749305434287\n",
      "Epoch: 0, chr1:6193152-6291456, loss: 0.8663109664496187\n",
      "Epoch: 0, chr1:6291456-6389760, loss: 0.7586016767193585\n",
      "Epoch: 0, chr1:6389760-6488064, loss: 0.7893474845788429\n",
      "Epoch: 0, chr1:6488064-6586368, loss: 0.7752143049716753\n",
      "Epoch: 0, chr1:6586368-6684672, loss: 0.7912515512369774\n",
      "Epoch: 0, chr1:6684672-6782976, loss: 1.2956728651002063\n",
      "Epoch: 0, chr1:6782976-6881280, loss: 0.8269140281750029\n",
      "Epoch: 0, chr1:6881280-6979584, loss: 0.8775308159363544\n",
      "Epoch: 0, chr1:6979584-7077888, loss: 1.8933930420841223\n",
      "Epoch: 0, chr1:7077888-7176192, loss: 0.8806791642138134\n",
      "Epoch: 0, chr1:7176192-7274496, loss: 1.4316465293774205\n",
      "Epoch: 0, chr1:7274496-7372800, loss: 0.8036404296585619\n",
      "Epoch: 0, chr1:7372800-7471104, loss: 1.3028171236490715\n",
      "Epoch: 0, chr1:7471104-7569408, loss: 1.4138040826051979\n",
      "Epoch: 0, chr1:7569408-7667712, loss: 3.2661883043988595\n",
      "Epoch: 0, chr1:7667712-7766016, loss: 1.4766048741585192\n",
      "Epoch: 0, chr1:7766016-7864320, loss: 0.8001083015816659\n",
      "Epoch: 0, chr1:7864320-7962624, loss: 0.7973604024748567\n",
      "Epoch: 0, chr1:7962624-8060928, loss: 0.7780256897457809\n",
      "Epoch: 0, chr1:8060928-8159232, loss: 1.1016722498939513\n",
      "Epoch: 0, chr1:8159232-8257536, loss: 1.2809607255554571\n",
      "Epoch: 0, chr1:8257536-8355840, loss: 1.092725132872815\n",
      "Epoch: 0, chr1:8355840-8454144, loss: 1.9834228078098852\n",
      "Epoch: 0, chr1:8454144-8552448, loss: 1.1423521789913391\n",
      "Epoch: 0, chr1:8552448-8650752, loss: 1.842851875517885\n",
      "Epoch: 0, chr1:8650752-8749056, loss: 0.8555085801522293\n",
      "Epoch: 0, chr1:8749056-8847360, loss: 0.8830469797679679\n",
      "Epoch: 0, chr1:8847360-8945664, loss: 2.1140356830919003\n",
      "Epoch: 0, chr1:8945664-9043968, loss: 0.7304968199755\n",
      "Epoch: 0, chr1:9043968-9142272, loss: 1.622761194870018\n",
      "Epoch: 0, chr1:9142272-9240576, loss: 2.095051726329015\n",
      "Epoch: 0, chr1:9240576-9338880, loss: 0.9807194153679285\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "bin_size = 200\n",
    "\n",
    "fa_fetcher = FastaFetcher(fa_file)\n",
    "bigwig_fetcher = BigwigFetcher(bigwig_files, bins=bin_size)\n",
    "target_region_iter = BedIterator(target_region_bed)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for region in target_region_iter:\n",
    "        chrom, start, end = region\n",
    "        seq = fa_fetcher.fetch_tensor(chrom, start, end).cuda()\n",
    "        target = bigwig_fetcher.fetch_tensor(chrom, start, end).cuda()\n",
    "        loss = model(seq, target=target)\n",
    "        print(f\"Epoch: {epoch}, {chrom}:{start}-{end}, loss: {loss}\")\n",
    "        loss.backward()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
